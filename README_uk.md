# Lecture Cleanup ШІ Pipeline (Українською)

Інструмент перетворює довгі стенограми лекцій у читабельний Markdown для Obsidian та баз знань.
Зберігає зміст без втрат. Виправляє пунктуацію, регістр, типові помилки розпізнавання.
Додає просту структуру. Підтримує контроль термінів між блоками тексту.

- Вхід: `.txt` (рядки, опційно з `[HH:MM:SS,mmm]`) або `.srt` (в розробці із за різних форматів).
- Вихід: один `.md` файл і файл звіту `.csv` з QC-метриками.
- Працює порціями (chunks) із перекриттям. Підтримує підказки для єдиного написання термінів між частинами.
- Має 3 режими редагування: `strict`, `normal`, `creative`.

## Як це працює (коротко)
1) Читаємо вхідний файл (`.txt` або `.srt`). Для `.txt` можна мати час у квадратних дужках на початку рядків.
2) Розбиваємо текст на блоки (chunk) з перекриттям. Рядки не ріжемо, якщо можливо.
3) До кожного блока додаємо лише «контекст» із попереднього фрагмента (тільки для читання).
   Контекст можу бути: `raw` (сирий текст), `cleaned` (очищений попередній фрагмент) або `none`.
4) Надсилаємо фрагмент у OpenAI із суворими підказками (prompts).
5) Для `.txt` із часом додаємо тайм-коди у заголовки фрагмента. Уточнення: таймкод ставиться один і той самий для всіх заголовків у межах одного чанка (бо береться початок блоку для простоти)
6) При зшиванні видаляємо дублікати на межі між блоками.
7) Збираємо інформацію про «злиття термінів» і підказуємо її наступним блокам. У коментарі `<!-- merged_terms: ... -->` лишаємо ТІЛЬКИ нові зміни цього блока.
8) Склеюємо всі блоки у фінальний Markdown. За бажанням додаємо неавторський підсумок наприкінці.
9) Пишемо QC-звіт (наскільки сильно змінився кожен шматок).

## Алгоритм (детальніше)
- Вхідні рядки:
  - TXT: кожен рядок зберігається. Рядки виду `[HH:MM:SS,mmm] текст` дають часові мітки для заголовків.
  - SRT: береться лише текст. Час для заголовків не додається. Рекомендовано конвертувати SRT→рядковий TXT з таймкодом на початку для повного функціоналу.
- Чанкінг (line-preserving):
  - `chunk_text_line_preserving(...)` збирає блоки до ліміту `txt_chunk_chars` із перекриттям `txt_overlap_chars`.
  - «Контекст» = останні `txt_overlap_chars` попереднього блоку (тільки читати, не виводити).
- Виклик OpenAI:
  - Системний промпт залежить від режиму: `strict` / `normal` / `creative`.
  - Користувацький шаблон містить: мову, список слів-паразитів, стиль для асайдів/жартів, термін-підказки `TERM_HINTS` (приховані), «контекст», та сам фрагмент.
- Нормалізація термінів між блоками:
  - Витягуємо `<!-- merged_terms: ... -->` з відповіді.
  - Зберігаємо глобальну мапу «канонічний → варіанти» для простот для простотии.
  - Передаємо її у наступні виклики як `TERM_HINTS` (JSON), але не показуємо в тексті.
  - У коментарі для поточного фрагмента залишаємо лише нові елементи цього блока (решта фільтрується).
- Тайм-коди:
  - Для TXT з позначеним часом — додаємо `[HH:MM:SS]` або посилання `[#t=HH:MM:SS]` у кінець заголовків фрагмента.
- Дедуплікація стику:
  - Порівнюємо кінець попереднього і початок поточного на вікні `stitch_dedup_window_chars`. Видаляємо дублікати з початку поточного.
- Підсумок:
  - За бажанням додаємо розділ із неавторським підсумком (окремий промпт).
- QC:
  - Записуємо CSV: довжини, схожість з оригіналом, частка змін.

## Встановлення
1) Встановіть Python 3.10+.
2) Створіть `.env` у корені проєкту (або скопіюйте `cp .env_default .env`:

   ```env
   OPENAI_API_KEY=ваш_ключ
   ```
3) Запустіть один раз ініціалізацію середовища:

   ```bash
   ./init_once.sh
   ```
   Скрипт створить `.venv` і поставить залежності (`pyyaml`, `openai`).

## Запуск
Рекомендується запускати через обгортки `.sh`. Вони активують `.venv` та викликають Python-скрипт із потрібними прапорцями.
!!!УВАГА, якщо не встановити останню версію openai, то програма дуже ймовірно не буде коректно працювати, або буде крашитися з помилками! УВАГА!!!

- Один файл:

  ```bash
  ./lecture_cleanup.sh --input input/lecture.txt --lang=uk
  ```

- Пакетна обробка всіх `.txt` у каталозі (за замовчуванням `./input`):

  ```bash
  ./bulk_cleanup.sh --lang=uk
  # або в іншій теці
  ./bulk_cleanup.sh --lang=uk --indir=./notes
  ```

Файли виходу зберігаються у `./output`:
- `lecture.md` — фінальний Markdown
- `lecture_qc_report.csv` — QC-звіт

## CLI-прапорці (основні)
Ці прапорці передаються до `scripts/run_pipeline.py` через `.sh`.

- `--input` (обов’язково): шлях до `.txt` або `.srt`.
- `--format`: `txt` або `srt` (інакше визначається за розширенням).
- `--outdir`: тека для вихідних файлів (за замовчуванням `output`).
- `--lang`: `ru`, `uk`, `en`.
- `--glossary`: шлях до файлу зі словником (по одному терміну в рядок).
- `--txt-chunk-chars`: розмір блока у символах (перекривається конфігом).
- `--txt-overlap-chars`: перекриття у символах.
- `--include-timecodes`: додати тайм-коди до заголовків (для TXT з часом).
- `--use-context-overlap {raw,cleaned,none}`: звідки брати контекст для наступного фрагмента.
- `--debug`: детальний лог (включно з промптами/відповідями).

Приклади:

```bash
# Базовий виклик (українська, TXT визначиться автоматично)
./lecture_cleanup.sh --input input/lec1.txt --lang=uk

# SRT без тайм-кодів у заголовках
(SRT ще в розробці)
./lecture_cleanup.sh --input input/lec1.srt --lang=uk --format srt

# Налаштування розміру та перекриття
./lecture_cleanup.sh --input input/lec1.txt --lang=uk \
  --txt-chunk-chars 6000 --txt-overlap-chars 600

# Звідки брати контекст для перекриття (overlap)
./lecture_cleanup.sh --input input/lec1.txt --lang=uk --use-context-overlap cleaned

# З глосарієм і тайм-кодами
./lecture_cleanup.sh --input input/lec1.txt --lang=uk --glossary data/my_glossary.txt --include-timecodes

# Увімкнути відлагодження
./lecture_cleanup.sh --input input/lec1.txt --lang=uk --debug
```

## Конфігурація (`config.yaml`)
Більшість опцій можна перекрити CLI-прапорцями.

- `language`: мова лекції (`ru`, `uk`, `en`). Необов’язково, бо вказуєте `--lang`.
- `model`: модель OpenAI (наприклад, `gpt-5.1`, `gpt-5-mini`).
- `temperature`: температура (0–2). Для режимів очищення краще помірні значення. Для GPT5 - завжди повинна бути 1
- `top_p`: top-p семплінг. Залишайте 1.0 для детермінізму.
- `format`: `txt` або `srt`. Якщо вказано — перекриває авто-визначення.
- `txt_chunk_chars`: розмір блока (символи). Типово 6500.
- `txt_overlap_chars`: перекриття між блоками (символи). Типово 500.
- `use_context_overlap`: джерело контексту: `raw`, `cleaned`, `none`. За замовчуванням `raw`. Сумісність: `true` ≡ `raw`, `false` ≡ `none`.
- `stitch_dedup_window_chars`: вікно (символи) для видалення дублів при зшиванні. `null` = як `txt_overlap_chars`, `0` = вимкнено.
- `include_timecodes_in_headings`: чи додавати тайм-коди у заголовки (для TXT з часом).
- `content_mode`: `strict` / `normal` / `creative`.
  - `strict`: тільки поверхневі виправлення. Не міняти порядок слів.
  - `normal`: легка читабельність. Мінімальні перестановки, якщо без зміни змісту.
  - `creative`: трохи сміливіше структурування. Нормалізація термінів за контекстом - пріоритезує контекстні докази над частотою (щоб уникнути частотних ASR-помилок).
- `suppress_edit_comments`: якщо `true`, HTML-коментарі наприкінці кожного блоку видаляються з фінального Markdown.
- `highlight_asides_style`: `italic` або `blockquote` для жартів/асайдів.
- `append_summary`: додати підсумок наприкінці документа.
- `summary_heading`: заголовок розділу з підсумком.
- `parasites`: шляхи до списків «слів-паразитів» по мовах.

### Overlap (Звідки брати текст для контекстного перекриття - Overlap)
- Бюджет `txt_overlap_chars` визначає максимальну довжину контексту.
- Вибір джерела: `raw` або `cleaned` (без HTML-коментарів). Якщо `cleaned` порожній, автоматично fallback до `raw` з WARN.
- Відсікання з кінця у такому порядку:
  1. Цілі рядки.
  2. Якщо остання лінія не влазить — частина цієї лінії по реченнях (набір `overlap.sentence_delimiters`, типово `.!?…`).
  3. Якщо перше речення довше бюджету — по словах; якщо слово довше бюджету — його хвіст у межах бюджету.
- Жодних службових маркерів; порядок природний; довжина ≤ бюджету.

## Контроль термінів між блоками
- Модель фіксує випадки нормалізації назв у коментарі `<!-- merged_terms: ... -->`.
- Пайплайн збирає ці дані та передає наступним блокам у полі `TERM_HINTS` (приховано).
- У коментарях кожного блока показуються лише нові зміни для цього блока.
- Якщо певний канонічний варіант з’явився пізніше з іншим написанням, система об’єднає їх у кластер і збереже єдину форму для підказок.

## Файлова структура
- `input/` — покладіть сюди `.txt` або `.srt`.
- `output/` — тут будуть `.md` і `_qc_report.csv`.
- `data/parasites_*.txt` — списки слів-паразитів для мов.
- `prompts/` — системні та користувацький шаблони підказок.
- `scripts/run_pipeline.py` — основна логіка.
- `scripts/slides_stub.py` — заготівка для додавання тексту/зображень слайдів (пізніше).
- `init_once.sh`, `lecture_cleanup.sh`, `bulk_cleanup.sh` — оболонки для запуску.

## Поради
- Створіть `.env` і не зберігайте ключ у Git.
- Для перевірки змін залишайте `suppress_edit_comments: false` у `config.yaml`.
- `--debug` допоможе у налаштуванні промптів та розборі відповідей.

## Обмеження
- SRT ще в процесі розробки і тестування, бо є декілька форматів. 
    - Поки-що обробляється без прив’язки тайм-кодів до заголовків.
    - Найімовірніше буде просто додано обробник, який буде стандартизувати всі файли в єдиний txt формат з таймкодами на початку.
- Узгодження термінів спирається на коментарі моделі. Якщо модель не зафіксувала нормалізацію, підказка не з’явиться.
- Підтримуються мови RU/UK/EN/DE (словники слів-паразитів). Без словників - можна будь-яку іншу мову використовувати
- Таймкоди до заголовків проставляються приблизні. Час береться з початку блоку на які розбивається текст (Chunk), тому всі заголовки в одному блоці матимуть один тайм-код. Якщо треба детальніше - зробіть менший розмір блоку.
- для генерації саммарі передається весь очищений текст однією порцією.
    - це  подвоює використання токенів моделі
    - може привести до крешу додатку, якщо запит буде довший за контекстне вікно моделі

## Ліцензія
This project is provided under the **MIT [LICENSE](LICENSE)** and is free WITHOUT WARRANTY OF ANY KIND.
