# Lecture Cleanup ШІ Pipeline (Українською)

Інструмент перетворює довгі стенограми лекцій у читабельний Markdown для Obsidian та баз знань.
Зберігає зміст без втрат. Виправляє пунктуацію, регістр, типові помилки розпізнавання.
Додає просту структуру. Підтримує контроль термінів між блоками тексту.

- Вхід: `.txt` (рядки, опційно з `[HH:MM:SS,mmm]`) або `.srt` (в розробці із за різних форматів).
- Вихід: один `.md` файл і файл звіту `.csv` з QC-метриками.
- Працює порціями (chunks) із перекриттям. Підтримує підказки для єдиного написання термінів між частинами.
- Має 3 режими редагування: `strict`, `normal`, `creative`.

## Як це працює (коротко)
1) Читаємо вхідний файл (`.txt` або `.srt`). Для `.txt` можна мати час у квадратних дужках на початку рядків.
2) Розбиваємо текст на блоки (chunk) з перекриттям. Рядки не ріжемо, якщо можливо.
3) До кожного блока додаємо лише «контекст» із попереднього фрагмента (тільки для читання).
   Контекст можу бути: `raw` (сирий текст), `cleaned` (очищений попередній фрагмент) або `none`.
4) Надсилаємо фрагмент у OpenAI із суворими підказками (prompts).
5) Для `.txt` із часом додаємо тайм-коди у заголовки фрагмента. Типово — таймкод початку блока для всіх заголовків; опційно `--process-timecodes-by-ai` передає сирі мітки моделі, щоб вона розставила тайм-коди для кожного заголовка окремо.
6) При зшиванні видаляємо дублікати на межі між блоками.
7) Збираємо інформацію про «злиття термінів» і підказуємо її наступним блокам. У коментарі `<!-- merged_terms: ... -->` лишаємо ТІЛЬКИ нові зміни цього блока.
8) Склеюємо всі блоки у фінальний Markdown. За бажанням додаємо неавторський підсумок наприкінці.
9) Пишемо QC-звіт (наскільки сильно змінився кожен шматок).

## Скріншоти

<img width="500" alt="cleanup-pipeline01" src="https://github.com/user-attachments/assets/9a096d90-fe5f-4c7a-8170-3dd25917ee8d" />
<img width="500" alt="cleanup-pipeline02" src="https://github.com/user-attachments/assets/20d11eda-0628-49cf-a987-62340eb19d77" />

---

<img width="500" alt="cleanup-pipeline03" src="https://github.com/user-attachments/assets/bb4cc7b1-eba5-401e-9b50-be5a65bd235e" />
<img width="500" alt="cleanup-pipeline04" src="https://github.com/user-attachments/assets/0d992cd8-ce0f-4f3a-8396-e96e09beeb2d" />

---

## Алгоритм (детальніше)
- Вхідні рядки:
  - TXT: кожен рядок зберігається. Рядки виду `[HH:MM:SS,mmm] текст` несуть тайм-коди; типово у всі заголовки блока ставиться час початку блока, або можна передати сирі мітки моделі (`--process-timecodes-by-ai`), щоб вона поставила тайм-коди для кожного заголовка окремо.
  - SRT: береться лише текст. Час для заголовків не додається. Рекомендовано конвертувати SRT→рядковий TXT з таймкодом на початку для повного функціоналу.
- Чанкінг (line-preserving):
  - `chunk_text_line_preserving(...)` збирає блоки до ліміту `txt_chunk_chars` із перекриттям `txt_overlap_chars`.
  - «Контекст» = останні `txt_overlap_chars` попереднього блоку (тільки читати, не виводити).
- Виклик OpenAI:
  - Системний промпт залежить від режиму: `strict` / `normal` / `creative`.
  - Користувацький шаблон містить: мову, список слів-паразитів, стиль для асайдів/жартів, термін-підказки `TERM_HINTS` (приховані), «контекст», та сам фрагмент.
- Нормалізація термінів між блоками:
  - Витягуємо `<!-- merged_terms: ... -->` з відповіді.
  - Зберігаємо глобальну мапу «канонічний → варіанти» для простот для простотии.
  - Передаємо її у наступні виклики як `TERM_HINTS` (JSON), але не показуємо в тексті.
  - У коментарі для поточного фрагмента залишаємо лише нові елементи цього блока (решта фільтрується).
- Тайм-коди:
  - Для TXT з позначеним часом — типово додаємо час початку блока у вигляді `[HH:MM:SS](#t=HH:MM:SS)` до кожного заголовка; опційно `--process-timecodes-by-ai` залишає сирі тайм-коди у фрагменті та просить LLM поставити таймкод для кожного заголовка й прибрати сирі мітки з тексту.
- Дедуплікація стику:
  - Порівнюємо кінець попереднього і початок поточного на вікні `stitch_dedup_window_chars`. Видаляємо дублікати з початку поточного.
- Підсумок:
  - За бажанням додаємо розділ із неавторським підсумком (окремий промпт).
- QC:
  - Записуємо CSV: довжини, схожість з оригіналом, частка змін.

## Встановлення
1) Встановіть Python 3.10+.
2) Створіть `.env` у корені проєкту (або скопіюйте `cp .env_default .env`:

   ```env
   OPENAI_API_KEY=ваш_ключ
   ```
3) Запустіть один раз ініціалізацію середовища:

   ```bash
   ./init_once.sh
   ```
   Скрипт створить `.venv` і поставить залежності (`pyyaml`, `openai`, `google-generativeai`).

## Запуск
Рекомендується запускати через обгортки `.sh`. Вони активують `.venv` та викликають Python-скрипт із потрібними прапорцями.
!!!УВАГА: використовуйте останню версію `openai`, інакше можливі збої чи некоректна робота. Для Gemini підтримки `init_once.sh` також ставить `google-generativeai`!!!

### Вибір LLM-провайдера (адаптери)

- Оберіть провайдера в `config.yaml` у полі `llm.provider` (типово: `openai`).
- Можна перевизначити через CLI: `--llm-provider openai|gemini|dummy|...`.
- API-ключі зберігайте у файлі `.env` в корені проєкту:
  - OpenAI: `OPENAI_API_KEY=...`
  - Gemini: `GOOGLE_API_KEY=...`

Ядро пайплайну не залежить від конкретного провайдера і спілкується через уніфікований адаптер. Щоб додати нового провайдера — скопіюйте `aiadapters/dummy_adapter.py`, реалізуйте `LLMAdapter.generate` і зареєструйте в `aiadapters/factory.py`.

### Рівні логування

- Конфіг: встановіть `logging.level` у `config.yaml` на `info`, `debug` або `trace`.
- CLI-перевизначення:
  - `--debug` → режим налагодження (без повних промптів/відповідей)
  - `--trace` → дуже детально; друкує повні промпти та відповіді LLM (великий обсяг, чутливі дані)

- Один файл:

  ```bash
  ./lecture_cleanup.sh --input input/lecture.txt --lang uk
  ```

- Пакетна обробка всіх `.txt` у каталозі (за замовчуванням `./input`):

  ```bash
  ./bulk_cleanup.sh --lang uk
  # або в іншій теці
  ./bulk_cleanup.sh --lang uk --indir ./notes
  ```

Файли виходу зберігаються у `./output`:
- `lecture.md` — фінальний Markdown
- `lecture_qc_report.csv` — QC-звіт

## CLI-прапорці (основні)
Ці прапорці передаються до `scripts/run_pipeline.py` через `.sh`.

- `--input` (обов’язково): шлях до `.txt` або `.srt`.
- `--format`: `txt` або `srt` (інакше визначається за розширенням).
- `--outdir`: тека для вихідних файлів (за замовчуванням `output`).
- `--lang`: `ru`, `uk`, `en`.
- `--glossary`: шлях до файлу зі словником (по одному терміну в рядок).
- `--txt-chunk-chars`: розмір блока у символах (перекривається конфігом).
- `--txt-overlap-chars`: перекриття у символах.
- `--include-timecodes`: додати тайм-коди до заголовків (для TXT з часом).
- `--process-timecodes-by-ai` / `--no-process-timecodes-by-ai`: передати сирі тайм-коди моделі, щоб вона сама розставила тайм-коди до кожного заголовка (TXT з рядками `[HH:MM:SS,mmm] ...`).
- `--use-context-overlap {raw,cleaned,none}`: звідки брати контекст для наступного фрагмента.
- `--debug`: дебаг-лог (без повних промптів/відповідей).
- `--trace`: дуже докладний лог; друкує повні промпти та відповіді.
- `--request-delay <секунди>`: пауза між LLM-запитами (0 вимикає).
- `--chunks <список>`: обробити лише вказані блоки; приклад: `1,3,7-9,23` (нумерація з 1)
- `--retry-attempts <N>`: повторювати невдалі LLM-запити до N разів (1 = без повторів)
- `--context-file <шлях>`: файл із контекстом для конкретного вводу; додається до КОРИСТУВАЦЬКОГО промпту відразу після загального речення "Context" (діє для всіх блоків). Можна вказувати кілька разів; блоки об’єднуються послідовно.

Приклади:

```bash
# Базовий виклик (українська, TXT визначиться автоматично)
./lecture_cleanup.sh --input input/lec1.txt --lang uk

# SRT без тайм-кодів у заголовках
(SRT ще в розробці)
./lecture_cleanup.sh --input input/lec1.srt --lang uk --format srt

# Налаштування розміру та перекриття
./lecture_cleanup.sh --input input/lec1.txt --lang uk \
  --txt-chunk-chars 6000 --txt-overlap-chars 600

# Звідки брати контекст для перекриття (overlap)
./lecture_cleanup.sh --input input/lec1.txt --lang uk --use-context-overlap cleaned

# З глосарієм і тайм-кодами
./lecture_cleanup.sh --input input/lec1.txt --lang uk --glossary data/my_glossary.txt --include-timecodes

# Тайм-коди в заголовках розставляє сама модель на основі сирих міток
./lecture_cleanup.sh --input input/lec1.txt --lang uk --include-timecodes --process-timecodes-by-ai

# Увімкнути режим налагодження (без повних текстів)
./lecture_cleanup.sh --input input/lec1.txt --lang uk --debug

# Увімкнути розширений трейс (повні промпти та відповіді)
./lecture_cleanup.sh --input input/lec1.txt --lang uk --trace
```

## Конфігурація (`config.yaml`)
Більшість опцій можна перекрити CLI-прапорцями.

Загальне
- `language`: мова лекції (`ru`, `uk`, `en`, `de`).
- `format`: `txt` або `srt` (перекриває авто-визначення).
- `txt_chunk_chars`: розмір блока (символи). Типово 6500.
- `txt_overlap_chars`: перекриття між блоками (символи). Типово 500.
- `use_context_overlap`: джерело контексту: `raw`, `cleaned`, `none`. За замовчуванням `raw`.
- `stitch_dedup_window_chars`: вікно (символи) для видалення дублів при зшиванні. `null` = як `txt_overlap_chars`, `0` = вимкнено.
- `include_timecodes_in_headings`: чи додавати тайм-коди у заголовки (для TXT з часом).
- `process_timecodes_by_ai`: залишати сирі тайм-коди у промпті та доручати моделі проставити тайм-коди для кожного заголовка (TXT із часом).
- `content_mode`: `strict` / `normal` / `creative`.
  - `strict`: тільки поверхневі виправлення. Не міняти порядок слів.
  - `normal`: легка читабельність. Мінімальні перестановки, якщо без зміни змісту.
  - `creative`: трохи сміливіше структурування. Нормалізація термінів за контекстом - пріоритезує контекстні докази над частотою (щоб уникнути частотних ASR-помилок).
- `suppress_edit_comments`: якщо `true`, HTML-коментарі наприкінці кожного блоку видаляються з фінального Markdown.
- `highlight_asides_style`: `italic` або `blockquote` для жартів/асайдів.
- `append_summary`: додати підсумок наприкінці документа.
- `summary_heading`: заголовок розділу з підсумком.
- `parasites`: шляхи до списків «слів-паразитів» по мовах.
- `llm.request_delay_seconds`: пауза між LLM-запитами (секунди); допомагає уникати лімітів; 0 вимикає.
- `retry.attempts`: глобальна кількість спроб (1 = без повторів)
- `retry.pause_seconds`: додаткова пауза, яка додається до запропонованої провайдером затримки; якщо немає пропозиції — використовується сама
- `llm.openai.retry.attempts`: спроби для OpenAI (перекриває глобальне)
- `llm.openai.retry.pause_seconds`: додаткова пауза для OpenAI (додається до поради провайдера; інакше використовується сама)
- `llm.gemini.retry.attempts`: спроби для Gemini (перекриває глобальне)
- `llm.gemini.retry.pause_seconds`: додаткова пауза для Gemini (додається до поради провайдера; інакше використовується сама)

LLM
- `llm.provider`: `openai`, `gemini` або власний адаптер.
- `llm.openai.model`: назва моделі (наприклад, `gpt-5-mini`).
- `llm.openai.temperature`: число (float).
- `llm.openai.top_p`: число або null.
- `llm.gemini.model`: назва моделі (наприклад, `gemini-2.5-pro`).
- `llm.gemini.temperature`: число (float).
- `llm.gemini.top_p`: число або null.
- `llm.request_delay_seconds`: пауза між LLM-запитами (секунди); допомагає уникати лімітів; 0 вимикає.

### Overlap (Звідки брати текст для контекстного перекриття - Overlap)
- Бюджет `txt_overlap_chars` визначає максимальну довжину контексту.
- Вибір джерела: `raw` або `cleaned` (без HTML-коментарів). Якщо `cleaned` порожній, автоматично fallback до `raw` з WARN.
- Відсікання з кінця у такому порядку:
  1. Цілі рядки.
  2. Якщо остання лінія не влазить — частина цієї лінії по реченнях (набір `overlap.sentence_delimiters`, типово `.!?…`).
  3. Якщо перше речення довше бюджету — по словах; якщо слово довше бюджету — його хвіст у межах бюджету.
- Жодних службових маркерів; порядок природний; довжина ≤ бюджету.

## Контроль термінів між блоками
- Модель фіксує випадки нормалізації назв у коментарі `<!-- merged_terms: ... -->`.
- Пайплайн збирає ці дані та передає наступним блокам у полі `TERM_HINTS` (приховано).
- У коментарях кожного блока показуються лише нові зміни для цього блока.
- Якщо певний канонічний варіант з’явився пізніше з іншим написанням, система об’єднає їх у кластер і збереже єдину форму для підказок.

## Файлова структура
- `input/` — покладіть сюди `.txt` або `.srt`.
- `output/` — тут будуть `.md` і `_qc_report.csv`.
- `data/parasites_*.txt` — списки слів-паразитів для мов.
- `prompts/` — системні та користувацький шаблони підказок.
- `scripts/run_pipeline.py` — основна логіка.
- `scripts/slides_stub.py` — заготівка для додавання тексту/зображень слайдів (пізніше).
- `init_once.sh`, `lecture_cleanup.sh`, `bulk_cleanup.sh` — оболонки для запуску.

## Поради
- Створіть `.env` і не зберігайте ключ у Git.
- Для перевірки змін залишайте `suppress_edit_comments: false` у `config.yaml`.
- `--trace` дозволяє дослідити повні промпти та відповіді; `--debug` дає корисні метадані без виводу повних текстів.

## Обмеження
- SRT ще в процесі розробки і тестування, бо є декілька форматів. 
    - Поки-що обробляється без прив’язки тайм-кодів до заголовків.
    - Найімовірніше буде просто додано обробник, який буде стандартизувати всі файли в єдиний txt формат з таймкодами на початку.
- Узгодження термінів спирається на коментарі моделі. Якщо модель не зафіксувала нормалізацію, підказка не з’явиться.
- Підтримуються мови RU/UK/EN/DE (словники слів-паразитів). Без словників - можна будь-яку іншу мову використовувати
- Таймкоди до заголовків проставляються приблизні (час початку блока), якщо не ввімкнути `--process-timecodes-by-ai`. Для точнішої прив’язки — менший розмір блока або режим AI-розстановки тайм-кодів.
- для генерації саммарі передається весь очищений текст однією порцією.
    - це  подвоює використання токенів моделі
    - може привести до крешу додатку, якщо запит буде довший за контекстне вікно моделі

## Ліцензія
This project is provided under the **MIT [LICENSE](LICENSE)** and is free WITHOUT WARRANTY OF ANY KIND.
