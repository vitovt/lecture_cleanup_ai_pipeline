# Global configuration for the lecture cleanup pipeline
# You can override most of these via CLI flags in run_pipeline.py

language: ru            # one of: ru, uk, en, de

# LLM settings (provider-aware). Configure provider and per-provider settings here.
llm:
  provider: gemini      # openai | gemini | dummy | ...
  # Optional delay (seconds) between consecutive LLM requests (chunks and summary).
  # Helps to avoid provider rate limits. 0 disables. CLI flag: --request-delay
  request_delay_seconds: 120
  openai:
    model: gpt-5-mini          # choose your model, e.g., gpt-5, gpt-5-mini, gpt-5-nano, gpt-5.1 / gpt-4.1 / o4-mini https://platform.openai.com/docs/models
    temperature: 1
    top_p: 1.0
    # Retry settings specific to OpenAI (override global defaults below).
    retry:
      attempts: 2        # 1 = no retry, 2..N = number of attempts per request
      pause_seconds: 10.0 # extra pause added to provider-suggested retry delay; if none, used alone
  gemini:
    model: gemini-2.5-pro   # model like: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite https://ai.google.dev/gemini-api/docs/models
    temperature: 1
    top_p: 1.0
    # Retry settings specific to Gemini (override global defaults below).
    retry:
      attempts: 3        # 1 = no retry, 2..N = number of attempts per request
      pause_seconds: 60.0 # extra pause added to provider-suggested retry delay; if none, used alone

# Global LLM API retry defaults (used if llm.<provider>.retry.* not provided). CLI --retry-attempts overrides attempts.
retry:
  attempts: 2        # 1 = no retry
  pause_seconds: 60.0 # extra pause added to provider-suggested retry delay; if none, used alone

# Input format control. When set, overrides auto-detection by file extension.
# One of: srt(VIP), txt (CLI --format still overrides this if provided)
format: txt

# Chunking
txt_chunk_chars: 6500   # chunk size for plain text (TXT)

# Overlap-specific options (advanced)
## Overlap typ
use_context_overlap: cleaned              # one of: raw, cleaned, none (backward-compatible with true/false)

## Number of characters from the END of the previous chunk passed to the model as READ-ONLY CONTEXT.
## Improves continuity; not included in output. Typical: 300–800 (0 = off).
txt_overlap_chars: 500  # overlap for plain text (TXT)

## Overlap sentence delimiters
overlap_sentence_delimiters: ".!?…"   # characters that mark end of sentences for tail selection

# Join-time window (in characters) used to remove any DUPLICATED prefix from the next chunk’s output.
# Usually set equal to txt_overlap_chars; increase if you still see duplicates.
# Try null, but can be completely disabled by setting 0
stitch_dedup_window_chars: null        # if null, defaults to txt_overlap_chars

# Output / behavior
include_timecodes_in_headings: true   # append timecodes to headings when available
process_timecodes_by_ai: true        # if true, keep timestamps in the prompt and ask the LLM to add per-heading timecodes
# QC report output:
# - outdir         : write next to the .md output (current behavior)
# - default_outdir : always write to ./output regardless of --outdir
# - off            : do not create a QC report
qc_report_mode: outdir

# Content optimization mode:
# Provide file name relative to prompts/system_ (without .md).
#
# Predefined modes:
# - normal   : balanced cleanup (default). Keeps meaning; light structure.
# - strict   : no reordering; only punctuation/casing; remove safe fillers.
# - creative : allows stronger rephrasing/merging short sentences; normalizes term variants.
# - creativedetailed : allows stronger rephrasing/merging short sentences; normalizes term variants but keep verbosity and original mood.
content_mode: creative

# Edit comments emitted at end of each output block (HTML comments like <!-- fixed: ... -->, <!-- merged_terms: ... -->).
# If true, these comments are removed from the final Markdown (still accounted internally by the model).
suppress_edit_comments: false

# Visual style for asides/jokes in the output.
# Allowed values:
# - italic      -> render as italics (*...*)
# - blockquote  -> render as blockquote (> ...)
highlight_asides_style: italic

#############################
# !!!      Warning      !!! #
# High token usage          #
############################# 
# The full result Markdown is sent to the model to produce the summary. It can double your expences.
append_summary: false
summary_heading: "## Summary (не авторський)"

# Paths to stopword lists (one per language)
parasites:
  ru: data/parasites_ru.txt
  uk: data/parasites_uk.txt
  en: data/parasites_en.txt
  de: data/parasites_de.txt

# Logging verbosity controls how much the pipeline prints:
#   - info  : concise progress + errors only; no stack traces; never prints full
#             LLM prompts/responses. Suitable for normal runs.
#   - debug : adds diagnostics (provider, model, temperature/top_p, chunk/overlap info)
#             and stack traces on errors; still does NOT print full prompts/responses.
#   - trace : prints EVERYTHING relevant, including full LLM requests and responses
#             for chunks and summary. This can be large and may contain sensitive
#             content. Use only for troubleshooting.
# You can override this via CLI flags:
#   --debug  -> sets level=debug
#   --trace  -> sets level=trace (implies debug)
logging:
  level: info
